#!/usr/bin/env  python
__license__   = 'GPL v3'
__copyright__ = '2008, Kovid Goyal kovid@kovidgoyal.net'
__docformat__ = 'restructuredtext en'

'''
www.guardian.co.uk
'''
import string
import re
from calibre import strftime
from calibre.ebooks.BeautifulSoup import BeautifulSoup, Tag
from calibre.web.feeds.news import BasicNewsRecipe

class Guardian(BasicNewsRecipe):

    title = u'The Guardian'
    __author__ = 'Seabound and Sujata Raman'
    language = 'en_GB'

    #oldest_article = 7
    #max_articles_per_feed = 100
    remove_javascript = True

    timefmt = ' [%a, %d %b %Y]'
    keep_only_tags = [
                      dict(name='div', attrs={'id':["content","article_header","main-article-info",]}),
                           ]
    remove_tags = [
                        dict(name='div', attrs={'class':["video-content","videos-third-column"]}),
                        dict(name='div', attrs={'id':["article-toolbox","subscribe-feeds",]}),
                        dict(name='ul', attrs={'class':["pagination"]}),
                        dict(name='ul', attrs={'id':["content-actions"]}),
                        ]
    use_embedded_content    = False

    no_stylesheets = True
    extra_css = '''
                    .article-attributes{font-size: x-small; font-family:Arial,Helvetica,sans-serif;}
                    .h1{font-size: large ;font-family:georgia,serif; font-weight:bold;}
                    .stand-first-alone{color:#666666; font-size:small; font-family:Arial,Helvetica,sans-serif;}
                    .caption{color:#666666; font-size:x-small; font-family:Arial,Helvetica,sans-serif;}
                    #article-wrapper{font-size:small; font-family:Arial,Helvetica,sans-serif;font-weight:normal;}
                    .main-article-info{font-family:Arial,Helvetica,sans-serif;}
                    #full-contents{font-size:small; font-family:Arial,Helvetica,sans-serif;font-weight:normal;}
                    #match-stats-summary{font-size:small; font-family:Arial,Helvetica,sans-serif;font-weight:normal;}
                '''
   
    def parse_index(self):            
           
            soup = self.index_to_soup('http://www.guardian.co.uk/theguardian')
             # find cover pic
            img = soup.find( 'img',attrs ={'alt':'Guardian digital edition'})        
            if img is None: return None
            else:
                self.cover_url = img['src']                
             # end find cover pic
            sections = []
            ans = []
            for li in soup.findAll( 'li'):
                section = ''
                articles = []
                                
                if li.a and li.a.has_key('href'):                
                        url =  li.a['href']                        
                        if 'mainsection' in url:                            
                            section = self.tag_to_string(url)
                            i = len(section)
                            
                            index1 = section.rfind('/',0,i)
                            section = section[index1+1:i]                            
                            sections.append(section)
                            
                            #find the articles in the Main Section  start                          
                            soup = self.index_to_soup(url)
                            date = strftime('%a, %d %b')
                            descl = []
                            
                            for desclist in  soup.findAll(name='div',attrs={'class':"trailtext"}):
                                descl.append(self.tag_to_string(desclist).strip())
                            
                            t = -1
                            for tag in soup.findAll('h3'):
                                t = t+1
                                
                                for a in tag.findAll('a'):
                                    
                                    if t < len(descl):                                   
                                        desc =  descl[t]
                                    else:
                                        desc = ''   
                                    if a and a.has_key('href'):                
                                        url2 =  a['href']                                        
                                    else:
                                        url2 =''                                    
                                    title = self.tag_to_string(a)
                                                                       
                                    if len(articles) == 0:    #First article
                                                                               
                                         articles.append({
                                                 'title':title,
                                                 'date':date,
                                                 'url':url2,
                                                 'description':desc,
                                                    })
                                    else:
                                        #eliminate duplicates start
                                        if {'title':title,'date':date,'url':url2,'description':desc} in articles : 
                                                url2 = ''
                                        #eliminate duplicates end
                                        else:
                                                if 'http://jobs.guardian.co.uk/' in url2:
                                                    url2 = ''
                                                else:
                                                    
                                                    articles.append({
                                                     'title':title,
                                                     'date':date,
                                                     'url':url2,
                                                     'description':desc,                                                     
                                                        })                                                
                            #find the articles in the Main Section end
                            ans.append( articles)
                            
                        else:
                                url =''
                        
                        
            titles = map(self.find_title, sections)
            ans1 = list(zip(titles,ans))
            
            return ans1[2:]
        
    def find_title(self, section):
        d = {'topstories':'Top Stories', 'international':'International', 'editorialsandreply':'Editorials and Reply',
             'commentanddebate':'Comment and Debate','uknews':'UK News','saturday':'Saturday','sunday':'Sunday',
                'reviews':'Reviews', 'obituaries':'Obituaries'}        
        
        return d.get(section, section)

    def preprocess_html(self, soup):
          
          for item in soup.findAll(style=True):
              del item['style']

          for item in soup.findAll(face=True):
              del item['face']
          for tag in soup.findAll(name=['ul','li']):
                tag.name = 'div'

          return soup






